{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3101767c-a250-4c1a-91fa-d24b2bf1b877",
   "metadata": {},
   "source": [
    "# Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90448426-97ee-4bdc-8397-6e43721b5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    ")\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6afb9050-3720-43fd-aeac-21d38134740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ua5292f0d5d5714d44bdef3ac4b25669/.local/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MarianMTModel(\n",
       "  (model): MarianModel(\n",
       "    (shared): Embedding(58905, 512, padding_idx=58904)\n",
       "    (encoder): MarianEncoder(\n",
       "      (embed_tokens): Embedding(58905, 512, padding_idx=58904)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianEncoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): SiLU()\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): MarianDecoder(\n",
       "      (embed_tokens): Embedding(58905, 512, padding_idx=58904)\n",
       "      (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x MarianDecoderLayer(\n",
       "          (self_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (activation_fn): SiLU()\n",
       "          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): MarianAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=58905, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load translated Taita-English dataset (assuming you've built it like before)\n",
    "# If it's not yet built, run your previous preprocessing script first.\n",
    "dataset = load_dataset(\"thinkKenya/kenyan-low-resource-language-data\", \"dav_swa\")\n",
    "\n",
    "# Load tokenizer and model (we'll fine-tune T5-small)\n",
    "model_name = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Translate Swahili to English using Rogendo\n",
    "rogendo_model_name = \"Rogendo/sw-en\"\n",
    "rogendo_tokenizer = AutoTokenizer.from_pretrained(rogendo_model_name)\n",
    "rogendo_model = AutoModelForSeq2SeqLM.from_pretrained(rogendo_model_name).to(device)\n",
    "rogendo_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fcf6187-283d-43bd-aba7-7d088cad3f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "254be5ad7a3146929ce1fe868d125a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadeba54691d40e18b3eda89ecd0c1c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_text': 'Hata iji Wavika', 'target_text': 'When You Are at Hand'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Translate Swahili to English using the Rogendo model\n",
    "def translate_swa_to_en(batch):\n",
    "    swahili_sentences = [item[\"swa\"] for item in batch[\"translation\"]]\n",
    "    inputs = rogendo_tokenizer(swahili_sentences, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = rogendo_model.generate(**inputs, max_length=128)\n",
    "    english_sentences = rogendo_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return {\"english\": english_sentences}\n",
    "\n",
    "# Apply translation on the train split\n",
    "translated_train = dataset[\"train\"].map(translate_swa_to_en, batched=True, batch_size=16)\n",
    "\n",
    "# Step 2: Build input-output pairs (Taita â†’ English)\n",
    "def build_input_target(example):\n",
    "    return {\n",
    "        \"input_text\": example[\"translation\"][\"dav\"],   # Taita\n",
    "        \"target_text\": example[\"english\"]              # Translated English\n",
    "    }\n",
    "\n",
    "# Apply mapping\n",
    "taita_en_dataset = translated_train.map(build_input_target)\n",
    "\n",
    "# Keep only necessary columns\n",
    "taita_en_dataset = taita_en_dataset.remove_columns(\n",
    "    [col for col in taita_en_dataset.column_names if col not in [\"input_text\", \"target_text\"]]\n",
    ")\n",
    "\n",
    "# Preview a sample\n",
    "print(taita_en_dataset[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d76d942-6681-41f0-8f1f-26e36dc59a27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdfc7abc4aa94e89af91ec7906432460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21329 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize for training\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"input_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    targets = tokenizer(example[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized = taita_en_dataset.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e51095-b4fd-434d-85ba-1798ed4bfc5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ua5292f0d5d5714d44bdef3ac4b25669/.local/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "[W311 17:34:34.307259733 OperatorEntry.cpp:155] Warning: Warning only once for all operators,  other operators may also be overridden.\n",
      "  Overriding a previously registered kernel for the same operator and the same dispatch key\n",
      "  operator: aten::_cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()\n",
      "    registered at /workspace/repositories/IPEX/pytorch/build/aten/src/ATen/RegisterSchema.cpp:6\n",
      "  dispatch key: XPU\n",
      "  previous kernel: registered at /workspace/repositories/IPEX/pytorch/build/aten/src/ATen/RegisterCPU.cpp:30476\n",
      "       new kernel: registered at /workspace/repositories/IPEX/ipex/build/Release/csrc/gpu/csrc/aten/generated/ATen/RegisterXPU.cpp:2971 (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-11 17:34:36,390] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to xpu (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# Set up training args with checkpointing\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    resume_from_checkpoint=True  # <<< This ensures it resumes if rerun\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956b2d5-399e-46e3-8a56-ab9f1828bc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 17:35:11,768 - _logger.py - IPEX - INFO - Currently split master weight for xpu only support sgd\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7200' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7200/7200 12:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Ter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.306500</td>\n",
       "      <td>0.282355</td>\n",
       "      <td>0.968597</td>\n",
       "      <td>0.150864</td>\n",
       "      <td>120.550162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.246800</td>\n",
       "      <td>0.237860</td>\n",
       "      <td>2.340730</td>\n",
       "      <td>0.219025</td>\n",
       "      <td>114.239482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.224600</td>\n",
       "      <td>0.225209</td>\n",
       "      <td>2.975336</td>\n",
       "      <td>0.245232</td>\n",
       "      <td>111.974110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ua5292f0d5d5714d44bdef3ac4b25669/.local/lib/python3.11/site-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[58904]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "2025-03-11 17:39:14,860 - absl - INFO - Using default tokenizer.\n",
      "2025-03-11 17:43:28,734 - absl - INFO - Using default tokenizer.\n",
      "2025-03-11 17:47:42,297 - absl - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7200, training_loss=0.29543087111579047, metrics={'train_runtime': 750.6268, 'train_samples_per_second': 76.72, 'train_steps_per_second': 9.592, 'total_flos': 1952139039473664.0, 'train_loss': 0.29543087111579047, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "\n",
    "bleu = evaluate.load(\"sacrebleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "ter = evaluate.load(\"ter\")\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])[\"score\"],\n",
    "        \"rougeL\": rouge.compute(predictions=decoded_preds, references=decoded_labels)[\"rougeL\"],\n",
    "        \"ter\": ter.compute(predictions=decoded_preds, references=decoded_labels)[\"score\"],\n",
    "    }\n",
    "\n",
    "split = tokenized.train_test_split(test_size=0.1)\n",
    "train_ds = split[\"train\"]\n",
    "eval_ds = split[\"test\"]\n",
    "\n",
    "eval_ds = eval_ds.select(range(100))\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./taita_en_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    generation_max_length=64,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=eval_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f7305f-19fb-4d42-9b9c-bec44ff468f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swahili: Habari za asubuhi.\n",
      "English : Good morning news.\n",
      "----------------------------------------\n",
      "Swahili: Jina langu ni Amina.\n",
      "English : My name is Amen.\n",
      "----------------------------------------\n",
      "Swahili: Ninapenda kusoma vitabu.\n",
      "English : I love to read books.\n",
      "----------------------------------------\n",
      "Swahili: Tafadhali nisaidie kuelewa hii.\n",
      "English : Please help me to understand this.\n",
      "----------------------------------------\n",
      "Swahili: Asante sana kwa msaada wako.\n",
      "English : Thank you so much for your help.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch # type: ignore\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"Rogendo/sw-en\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "swahili_sentences = [\n",
    "    \"Habari za asubuhi.\",\n",
    "    \"Jina langu ni Amina.\",\n",
    "    \"Ninapenda kusoma vitabu.\",\n",
    "    \"Tafadhali nisaidie kuelewa hii.\",\n",
    "    \"Asante sana kwa msaada wako.\"\n",
    "]\n",
    "\n",
    "translation_pipeline = pipeline(\"translation\", model=model, tokenizer=tokenizer, src_lang=\"sw\", tgt_lang=\"en\", device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "for sentence in swahili_sentences:\n",
    "    result = translation_pipeline(sentence)\n",
    "    print(f\"Swahili: {sentence}\")\n",
    "    print(f\"English : {result[0]['translation_text']}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19404ea8-e9ca-49d0-ac6a-3628d399fac7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 18:13:26,871 - absl - INFO - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22520889341831207, 'eval_bleu': 2.975336211265933, 'eval_rougeL': 0.24518282834969118, 'eval_ter': 111.97411003236246, 'eval_runtime': 13.9611, 'eval_samples_per_second': 7.163, 'eval_steps_per_second': 1.791, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate()\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6148b1-a7ad-44b7-a0cc-70a83dd6c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use xpu:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Wavi wijali kulela wana.\n",
      "Translation: Parents are quick to children.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\n",
    "    \"translation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "input_text = \"Wavi wijali kulela wana.\"\n",
    "\n",
    "result = translator(input_text, max_length=64)\n",
    "\n",
    "print(\"Input:\", input_text)\n",
    "print(\"Translation:\", result[0]['translation_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef8e02-9eb5-4f6a-986e-56551f576c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Wavi wijali kulela wana.\n",
      "Translation: Parents are quick to children.\n",
      "\n",
      "Input: Wana ni inosi kufuma kwa mlungu.\n",
      "Translation: Children are a gift from God.\n",
      "\n",
      "Input: Hata iji Wavika\n",
      "Translation: Even if they're on the move\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Wavi wijali kulela wana.\",\n",
    "    \"Wana ni inosi kufuma kwa mlungu.\",\n",
    "    \"Hata iji Wavika\"\n",
    "]\n",
    "\n",
    "\n",
    "'''\n",
    "Actual Translations of the texts list\n",
    "[\n",
    "    Parents care about the upbringing of children.\n",
    "    Children are a gift from God.\n",
    "    When you arrive\n",
    "]\n",
    "'''\n",
    "\n",
    "for text in texts:\n",
    "    result = translator(text, max_length=64)\n",
    "    print(f\"\\nInput: {text}\")\n",
    "    print(f\"Translation: {result[0]['translation_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a3048b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
